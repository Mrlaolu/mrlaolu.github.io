{"posts":[{"title":"RL概论学习笔记","text":"RL概论视频网址：李宏毅 强化学习 (Reinforcement Learning, RL) 2021_哔哩哔哩_bilibili What is RL？解决的问题收集标注很难，找不到最佳的解决方法 如何实现 RL跟ML三步的关系Step1 —— Function with unknown 定义Actor Input：复杂的输入（画面） Actor：以往是简单的table，现在一般是network Output：能进行的操作（Classification Task） 多少采取随机Sample 而不是直接取最大值作为这一步操作 Step2 —— Define “Loss” episode：一个游戏过程 Total reward(return)：$R = \\sum{r_t}$ (单个episode等到一个$r_t$) ——&gt; 去最大化 Loss可以看作负的Total reward Step3 —— Optimization 找一个Actor参数，让 $R(t)$ 越大越好 Policy Gradient 需要收集一些数据，指定有些行为希望他执行，有些希望不希望他执行 Reward delay：需要牺牲一些短期reward去得到长期更多的reward 一个操作的Reward：从目前到结尾的收益加权累加，并减去baseline 不同的RL很多在 $A_{i}$ 的定义上下文章 每一次epoch都要收集一次data，这里开销很大 Off-policy 能用一些方法，用 $\\theta^{i-1}$ 的资料训练 $\\theta^{i}$ ——&gt; PPO （要能知道actor to train跟actor to interact的差距） Exploration：收集数据的过程要有随机性——&gt; 收集到比较丰富的资料。 可以加大没随机到的概率；直接加noise Actor-CriticCritic：评估Actor的好坏 Value function $V^\\theta(s) $： 看到s后，用 $\\theta$ 作为Actor操作后得到的discounted cumulated reward 是多少 如何训练 Value function蒙特卡洛(MC)方法：直接观察玩完 $s$ 得到的 cumulated reward，然后与其逼近 Temporal-difference(TD)方法： 收集到 $r_t$ 后，直接通过自己这一项和下一项 * $\\gamma$ 的差值去逼近（好处：不用玩完游戏，适合游戏时间较长的游戏 ） 一个方法是将baseline设为 $V^{\\theta}(s_1)$ 然后将当前得到的 数据 $G^{‘}_{t}$ 与 其做差查看好坏 Advantage Actor-Critic 将 执行过操作 $a_t$ 得到的reward（图下方） 与 随机抽样得到的reward 比较，如果大于则认为较好，否则认为较差。 DQN待深入了解 Reward Shaping如果多数的reward都是0怎么办 ——&gt; 提供额外的reward去学习（Reward shaping） 就是通过人工加入规则外的reward（如射击游戏里浪费子弹就要扣分）——&gt; 需要设计者对问题本身有足够的理解 Curiosity based reward shaping要求agent看到新的东西，而且是有意义的新 就给他reward No Reward: Learning from Demonstration定义Reward的方式非常困难，没定义好Reward会出现很坏的情况 Imitation Learning 模仿学习 Inversed Reinforcement Learning本来不知道reward，通过expert示范去反推Reward Function 简单的Reward Function可能能训出复杂的actor 先获得一些数据，然后找到一个reward function让expert的reward高于现有的actor，再让actor在这个reward function上学习 GAN VS IRL框架类似","link":"/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"论文笔记：CLIP","text":"论文笔记：CLIP论文原文：Learning Transferable Visual Models From Natural Language Supervision 视频导读：CLIP 论文逐段精读【论文精读】_哔哩哔哩_bilibili 特性：迁移性好，利用好自然语言的监督信号，训练规模大 CLIP理论采用对比学习：将Image和Text编码，配对的为正样本，不配对的为负样本 为了让ImageNet的标签变成句子，通过 A photo of a {object} 这个prompt template 去形成句子，比单个标签效果好 ZeroShot实现：通过图片与文本算相似度的方式，去判断照片里有哪些物体 因为将图像的语义和文字的语义结合在一起，所有他学到的特征语义性强且迁移性好 Linear Probe（线性探测） 冻结骨干网络（Backbone），只训练分类头 如何做：在输出端构建一个全连接层，输出为想要的分类 为什么：计算效率快、避免灾难性遗忘、研究 Zero-shot vs Linear Probe 差距 CLIP方法 先对image和text进行编码 通过归一化将image和text投射到同一个空间里去（Embedding） 算出相似度 相似度跟ground truth计算交叉熵，计算出loss EfficientNet 如何放大模型 深度 (Depth) $\\times \\alpha^\\phi$ 宽度 (Width) $\\times \\beta^\\phi$ 分辨率 (Resolution) $\\times \\gamma^\\phi$ 这里 $\\alpha, \\beta, \\gamma$ 是固定的常数。改变 $\\phi$ 去放大模型 CLIP实验Zero-Shot Transfer以前的学习是在学一些泛化性好的特征——&gt; 还是要微调，会发生distribution shift 直接用又大又好的模型进行覆盖 文本和图像编码embedding后直接算相似度，相似度最大的就是结果 Prompt Engineering and Ensembling通过类似 A photo of a {label} 的样式，将单词变成句子减少歧义性，从而有更好的效果 启示：如果你预先知道一些有关信息，如知道这个数据集里的都是宠物，可以在构造的句子里加上，能缩小解空间，提升正确性 Zero-Shot CLIP Performance对于特别难描述或者本身较难的任务（如数个数，纹理分类），以及特定领域知识的任务（如肿瘤分类），Zero-shot表现较差，需要few-shot。 Representation Learning为什么不用fine-tune而是用Linear Probe 本身CLIP在研究跟数据集无关的预训练方式，微调后验证不了预训练模型的好坏（因为可能在微调过程中，预训练模型变强或者变差了），用Linear Probe学习空间就比较少。 fine-tune要搜超参（比如大数据集要用大学习率，否则用小的） CLIP 局限性 在一些任务上与当时SOTA有区别，能差上十几个点 虽然模型扩大还能提高正确率，但是扩大后训练代价太大 有些数据集如细分类，抽象概念等做的不好 在MNIST上表现差（数据集里几乎没有数字） 数据利用率低——&gt; 可以做数据增强,自监督,伪标签的方法 无zero-shot数据集,会让研发有针对性(超参啥的) 提供oneshot,fewshot等情况下,可能还没有zeroshot好 伪标签 (Pseudo-Labeling):利用无标签数据 &amp; 清洗脏数据(通过教师模型) 自监督学习(对比学习):除了图文对比,还可以有图图对比(数据增强后对齐)和掩码重建","link":"/2025/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ACLIP/"},{"title":"ResNet学习笔记","text":"ResNet学习笔记 ResNet亮点 超深网络架构（Over 1000 layer） 提出residual模块 使用Batch Normalization加速训练（丢弃dropout） 为什么层数越高，效果越差？ 梯度消失或梯度爆炸 如何解决：数据标准化，权重初始化，Batch Normalization 退化问题（degradation problem） 如何解决：残差网络（residual） Residual 因为是相加，所以主分支与shortcut的输出特征矩阵shape必须相同 Batch Normalization去调整一批数据的feature map的分布，而不是某一张图片 其中 $\\gamma$ 和 $\\beta$ 是用来调整最后得到的均值和方差的，这两个需要学习 需要注意的地方： 训练时要将traning参数设置为True，在验证时将trainning参数设置为False。在pytorch中可通过创建模型的model.train()和model.eval()方法控制。 batch size尽可能设置大点，设置小后表现可能很糟糕，设置的越大求的均值和方差越接近整个训练集的均值和方差。 建议将bn层放在卷积层（Conv）和激活层（例如Relu）之间，且卷积层不要使用偏置bias，因为没有用（偏置完后归一化，偏置就不存在了） 迁移学习优势 能快速训练出一个理想结果 当数据集较小也能训练出理想效果 注意：要注意别人预处理的方式 因为一些网络的浅层是比较通用的，所以可以迁移到其他网络里去","link":"/2025/12/12/ResNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"Transformer学习笔记","text":"Transformer学习笔记 Self-Attention每个元素 $x_{i}$通过embedding得到 $a_{i}$ ,再通过 $a_{i}$ 乘上$W_{q},W_{k},W_{v}$ 得到 $q^{i},k^i,v^i$ $Q(query)$ : 去匹配每一个key $K(key):$ 去被query匹配 $V(value)$ : 需要被提取走的信息(a当中学习到的信息) 点乘：可以体现矩阵的相似度 缩放点积注意力（Scaled Dot-Prodect Attension） $a_{1,i} = q^1 \\cdot k^i / \\sqrt{d}$ … d 是 k 的维度 个人理解：$Q \\cdot K$ 计算出相似度，然后通过softmax，算出每个元素与我($k^{i}$)的相关概率 再通过这种相似概率($\\hat a_{i,j}$) 乘以信息 $v_{j}$ 得到输出 $b_{i}$ ， Multi-head Self-Attention将 $q$ 进行拆分成 $q_{1,1} q_{1,2}…$ 将其他参数也这样做，然后取出下标 $i$ 形成head $i$，获得多个注意力头 以此类推 所有同一种 $j$ 的 $q_{i,j},k_{i,j},v_{i,j}$ 生成一个 $b_{i,j}$ , 同一种 $j$ 的 $b_{j}$ 称为 head $j$ 将head进行拼接(Concat)，通过 $W^{o}$ 进一步融合，得出输出 Positional Encoding无序性（Permutation Invariance） 如果不对位置编码的话，在不同元素进行调换得出的结果是一样的 可以通过下图两种方法进行计算","link":"/2025/12/15/Transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"Vision Transformer学习笔记","text":"Vision Transformer学习笔记总览： 先分把图像切成几个patches（比如每个patches大小为16*16） 让patches通过 Linear Projection of Flattened Patches（embedding层） 获得 token 加上位置信息 在最前面加上一个[class]token（参考BERT），用于classification 经过Transfomer Encoder，提取[class]的输出 最后通过MLP层得到最终输出 Embedding层 transformer要求输入token（向量）序列（[num_token,token_dim]），跟图片维度[H,W,C]不符，需要embedding进行变换 以ViT-B/16为例和（224,224）照片为例，按照 16 * 16 划分得到 196 个Patches，每个patches[16,16,3]通过映射得到长度768的token。 代码实现直接通过如图所示的参数(kernal_size=16,stride=16,out_channel=768 )，获得[196,768]的矩阵 然后加上[class]token（矩阵Cat([1, 768], [196, 768]) -&gt; [197, 768]）以及叠加Position Embedding（1D Pos. Emb. 直接叠加到tokens上） Position Embedding Position Embedding形式不重要，但是有没有很重要（差了3个点） 因为1D Pos. Emd.计算简单，便采用 Transformer Encoder多个Encoder Block构成Transformer Encoder 一个Encoder Block里有两小层，核心分别为多头注意力和MLP，多个Encoder Block构成Transformer Encoder LayerNorm 针对自然语言处理领域提出的，应对时序的长度并不是一个定值（如每句话长度不同），难以使用BN LN是单个数据的指定维度进行Norm处理（与Batch无关），BN是对于一个batch数据的每个channel进行Norm处理 原文使用的是Dropout，一些大佬用的是DropPath，可能后者效果会好一点 MLP Block里面是全连接+GELU激活函数+Dropout，第一个全连接层会把输入节点个数翻4倍[197, 768] -&gt; [197, 3072]，第二个全连接层会还原回原节点个数[197, 3072] -&gt; [197, 768] MLP Head 原论文中说在训练ImageNet21K时是由Linear+tanh激活函数+Linear组成。 但是迁移到ImageNet1K上或者自己的数据上时，只用一个Linear即可。 ViT-B/16架构","link":"/2025/12/15/Vision%20Transformer/"}],"tags":[{"name":"学习笔记","slug":"学习笔记","link":"/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"强化学习","slug":"强化学习","link":"/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"深度学习","slug":"深度学习","link":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","link":"/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"课内","slug":"课内","link":"/tags/%E8%AF%BE%E5%86%85/"}],"categories":[],"pages":[]}