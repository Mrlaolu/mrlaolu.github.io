<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>标签: 学习笔记 - Mrlaolu&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Mrlaolu&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Mrlaolu&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Mrlaolu&#039;s Blog"><meta property="og:url" content="http://example.com/"><meta property="og:site_name" content="Mrlaolu&#039;s Blog"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://example.com/img/og_image.png"><meta property="article:author" content="Mrlaolu"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://example.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://example.com"},"headline":"Mrlaolu's Blog","image":["http://example.com/img/og_image.png"],"author":{"@type":"Person","name":"Mrlaolu"},"publisher":{"@type":"Organization","name":"Mrlaolu's Blog","logo":{"@type":"ImageObject","url":"http://example.com/img/logo.svg"}},"description":""}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.1.1"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="Mrlaolu&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags/">标签</a></li><li class="is-active"><a href="#" aria-current="page">学习笔记</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-11-30T06:23:54.268Z" title="2025/11/30 14:23:54">2025-11-30</time>发表</span><span class="level-item"><time dateTime="2025-11-30T06:34:33.827Z" title="2025/11/30 14:34:33">2025-11-30</time>更新</span><span class="level-item">7 分钟读完 (大约982个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ACLIP/">论文笔记：CLIP</a></p><div class="content"><h1 id="论文笔记：CLIP"><a href="#论文笔记：CLIP" class="headerlink" title="论文笔记：CLIP"></a>论文笔记：CLIP</h1><p>论文原文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a></p>
<p>视频导读：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1SL4y1s7LQ/?spm_id_from=333.337.search-card.all.click&vd_source=8850f9cfde09fd05c8380abaf11644b9">CLIP 论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p>
<p>特性：迁移性好，利用好自然语言的监督信号，训练规模大</p>
<h2 id="CLIP理论"><a href="#CLIP理论" class="headerlink" title="CLIP理论"></a>CLIP理论</h2><p>采用对比学习：将Image和Text编码，配对的为正样本，不配对的为负样本</p>
<p>为了让ImageNet的标签变成句子，通过 <code>A photo of a {object}</code> 这个prompt template 去形成句子，比单个标签效果好</p>
<p>ZeroShot实现：通过图片与文本算相似度的方式，去判断照片里有哪些物体</p>
<p>因为将图像的语义和文字的语义结合在一起，所有他学到的特征语义性强且迁移性好</p>
<blockquote>
<p>Linear Probe（线性探测）</p>
<p>冻结骨干网络（Backbone），只训练分类头</p>
<p>如何做：在输出端构建一个全连接层，输出为想要的分类</p>
<p>为什么：计算效率快、避免灾难性遗忘、研究 Zero-shot vs Linear Probe 差距</p>
</blockquote>
<h2 id="CLIP方法"><a href="#CLIP方法" class="headerlink" title="CLIP方法"></a>CLIP方法</h2><p><img src="/2025/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ACLIP/image-20251129215943792.png" alt="CLIP伪代码"></p>
<ol>
<li>先对image和text进行编码</li>
<li>通过归一化将image和text投射到同一个空间里去（Embedding）</li>
<li>算出相似度</li>
<li>相似度跟ground truth计算交叉熵，计算出loss</li>
</ol>
<blockquote>
<p>EfficientNet 如何放大模型</p>
<ul>
<li><strong>深度 (Depth) $\times \alpha^\phi$</strong></li>
<li><strong>宽度 (Width) $\times \beta^\phi$</strong></li>
<li><strong>分辨率 (Resolution) $\times \gamma^\phi$</strong></li>
</ul>
<p>这里 $\alpha, \beta, \gamma$ 是固定的常数。改变 $\phi$  去放大模型</p>
</blockquote>
<h2 id="CLIP实验"><a href="#CLIP实验" class="headerlink" title="CLIP实验"></a>CLIP实验</h2><h3 id="Zero-Shot-Transfer"><a href="#Zero-Shot-Transfer" class="headerlink" title="Zero-Shot Transfer"></a>Zero-Shot Transfer</h3><p>以前的学习是在学一些泛化性好的特征——&gt; 还是要微调，会发生distribution shift</p>
<p>直接用又大又好的模型进行覆盖</p>
<p><img src="/2025/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ACLIP/image-20251130125048947-1764478267387-1.png" alt="Zero-Shot推理"></p>
<p>文本和图像编码embedding后直接算相似度，相似度最大的就是结果</p>
<h3 id="Prompt-Engineering-and-Ensembling"><a href="#Prompt-Engineering-and-Ensembling" class="headerlink" title="Prompt Engineering and Ensembling"></a>Prompt Engineering and Ensembling</h3><p>通过类似 <code>A photo of a {label}</code> 的样式，将单词变成句子减少歧义性，从而有更好的效果</p>
<p>启示：如果你预先知道一些有关信息，如知道这个数据集里的都是宠物，可以在构造的句子里加上，能缩小解空间，提升正确性</p>
<h3 id="Zero-Shot-CLIP-Performance"><a href="#Zero-Shot-CLIP-Performance" class="headerlink" title="Zero-Shot CLIP Performance"></a>Zero-Shot CLIP Performance</h3><p>对于特别难描述或者本身较难的任务（如数个数，纹理分类），以及特定领域知识的任务（如肿瘤分类），Zero-shot表现较差，需要few-shot。</p>
<h3 id="Representation-Learning"><a href="#Representation-Learning" class="headerlink" title="Representation Learning"></a>Representation Learning</h3><p>为什么不用fine-tune而是用Linear Probe</p>
<ol>
<li>本身CLIP在研究跟数据集无关的预训练方式，微调后验证不了预训练模型的好坏（因为可能在微调过程中，预训练模型变强或者变差了），用Linear Probe学习空间就比较少。</li>
<li>fine-tune要搜超参（比如大数据集要用大学习率，否则用小的）</li>
</ol>
<h3 id=""><a href="#" class="headerlink" title=""></a></h3><h2 id="CLIP-局限性"><a href="#CLIP-局限性" class="headerlink" title="CLIP 局限性"></a>CLIP 局限性</h2><ol>
<li><p>在一些任务上与当时SOTA有区别，能差上十几个点</p>
</li>
<li><p>虽然模型扩大还能提高正确率，但是扩大后训练代价太大</p>
</li>
<li><p>有些数据集如细分类，抽象概念等做的不好</p>
</li>
<li><p>在MNIST上表现差（数据集里几乎没有数字）</p>
</li>
<li><p>数据利用率低——&gt; 可以做数据增强,自监督,伪标签的方法</p>
</li>
<li><p>无zero-shot数据集,会让研发有针对性(超参啥的)</p>
</li>
<li><p>提供oneshot,fewshot等情况下,可能还没有zeroshot好</p>
</li>
</ol>
<blockquote>
<p>伪标签 (Pseudo-Labeling):利用无标签数据 &amp; 清洗脏数据(通过教师模型)</p>
<p>自监督学习(对比学习):除了图文对比,还可以有图图对比(数据增强后对齐)和掩码重建</p>
</blockquote>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2025-11-29T07:37:16.000Z" title="2025/11/29 15:37:16">2025-11-29</time>发表</span><span class="level-item"><time dateTime="2025-11-29T11:07:47.411Z" title="2025/11/29 19:07:47">2025-11-29</time>更新</span><span class="level-item">8 分钟读完 (大约1197个字)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">RL概论学习笔记</a></p><div class="content"><h1 id="RL概论"><a href="#RL概论" class="headerlink" title="RL概论"></a>RL概论</h1><p>视频网址：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1rL411j7wG/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=8850f9cfde09fd05c8380abaf11644b9">李宏毅 强化学习 (Reinforcement Learning, RL) 2021_哔哩哔哩_bilibili</a></p>
<h2 id="What-is-RL？"><a href="#What-is-RL？" class="headerlink" title="What is RL？"></a>What is RL？</h2><h3 id="解决的问题"><a href="#解决的问题" class="headerlink" title="解决的问题"></a>解决的问题</h3><p>收集标注很难，找不到最佳的解决方法</p>
<h3 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h3><p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7BC34DCCE1-6EAD-45D8-BE07-1BBC788D9206%7D-1764341844928-2.png" alt="RL基本框架"></p>
<h3 id="RL跟ML三步的关系"><a href="#RL跟ML三步的关系" class="headerlink" title="RL跟ML三步的关系"></a>RL跟ML三步的关系</h3><h4 id="Step1-——-Function-with-unknown-定义Actor"><a href="#Step1-——-Function-with-unknown-定义Actor" class="headerlink" title="Step1 —— Function with unknown  定义Actor"></a>Step1 —— Function with unknown  定义Actor</h4><p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7BC6FE8E57-FAD4-4E2A-BEEE-E14D0BE64CD1%7D.png" alt="Function with unknown"></p>
<p>Input：复杂的输入（画面）</p>
<p><strong>Actor</strong>：以往是简单的table，现在一般是network</p>
<p>Output：能进行的操作（Classification Task）  多少采取随机Sample  而不是直接取最大值作为这一步操作</p>
<h4 id="Step2-——-Define-“Loss”"><a href="#Step2-——-Define-“Loss”" class="headerlink" title="Step2 —— Define “Loss”"></a>Step2 —— Define “Loss”</h4><p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7BD3027FC1-D63F-4060-8013-5A4F74E2B360%7D.png" alt="Define Loss"></p>
<p><strong>episode</strong>：一个游戏过程</p>
<p><strong>Total reward(return)</strong>：$R &#x3D; \sum{r_t}$ (单个episode等到一个$r_t$) ——&gt; 去最大化</p>
<p>Loss可以看作负的Total reward</p>
<h4 id="Step3-——-Optimization"><a href="#Step3-——-Optimization" class="headerlink" title="Step3 —— Optimization"></a>Step3 —— Optimization</h4><p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7B1775609E-399E-4D45-8222-FD9A04328A60%7D.png" alt="Optimization"></p>
<p>找一个Actor参数，让  $R(t)$  越大越好</p>
<hr>
<h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7BB94D68E1-255E-44AE-9189-6F8BA3EA1530%7D.png" alt="收集数据的基本概念图"></p>
<p>需要收集一些数据，指定有些行为希望他执行，有些希望不希望他执行</p>
<p><strong>Reward delay</strong>：需要牺牲一些短期reward去得到长期更多的reward</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7B97ECD0BC-273B-4165-9514-D0FC17D1A095%7D.png" alt="如何计算Reward的一个版本"></p>
<p>一个操作的Reward：从目前到结尾的收益加权累加，并减去baseline</p>
<p>不同的RL很多在 $A_{i}$ 的定义上下文章</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7B8B67AB80-DF46-40D3-B457-C5651D5CD954%7D.png" alt="梯度流程图"></p>
<p>每一次epoch都要收集一次data，这里开销很大</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7BB2D52225-3992-4936-A8D1-FAF6A558118C%7D.png" alt="On-policy VS Off-policy"></p>
<p><strong>Off-policy</strong> 能用一些方法，用 $\theta^{i-1}$ 的资料训练 $\theta^{i}$ ——&gt; PPO （要能知道actor to train跟actor to interact的差距）</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7B967A2A57-F562-47C6-981D-2154D9109DAF%7D.png" alt="Exploration"></p>
<p>Exploration：收集数据的过程要有随机性——&gt; 收集到比较丰富的资料。 </p>
<p>可以加大没随机到的概率；直接加noise</p>
<hr>
<h2 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h2><p><strong>Critic</strong>：评估Actor的好坏</p>
<p><strong>Value function</strong> $V^\theta(s) $： 看到s后，用 $\theta$ 作为Actor操作后得到的discounted cumulated reward 是多少</p>
<h4 id="如何训练-Value-function"><a href="#如何训练-Value-function" class="headerlink" title="如何训练 Value function"></a>如何训练 Value function</h4><p><strong>蒙特卡洛(MC)方法</strong>：直接观察玩完 $s$  得到的 cumulated reward，然后与其逼近</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20251129103914971.png" alt="MC"></p>
<p><strong>Temporal-difference(TD)方法</strong>：  收集到 $r_t$ 后，直接通过自己这一项和下一项 * $\gamma$ 的差值去逼近（好处：不用玩完游戏，适合游戏时间较长的游戏 ）</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7B7BBE71AF-DC8D-465D-B24F-8EBCEA9AEF2F%7D.png" alt="TD"></p>
<p>一个方法是将baseline设为  $V^{\theta}(s_1)$ 然后将当前得到的 数据 $G^{‘}_{t}$  与 其做差查看好坏</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20251129114735478.png" alt="收集数据的第3.5版本"></p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20251129115517486.png" alt="收集数据的第3.5版本"></p>
<h4 id="Advantage-Actor-Critic"><a href="#Advantage-Actor-Critic" class="headerlink" title="Advantage Actor-Critic"></a>Advantage Actor-Critic</h4><p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7BF59748C3-D338-4F0B-A487-C042CD572572%7D.png" alt="收集数据的第4版本"></p>
<p>将 执行过操作 $a_t$ 得到的reward（图下方） 与 随机抽样得到的reward 比较，如果大于则认为较好，否则认为较差。 </p>
<h4 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h4><p>待深入了解</p>
<hr>
<h2 id="Reward-Shaping"><a href="#Reward-Shaping" class="headerlink" title="Reward Shaping"></a>Reward Shaping</h2><p>如果多数的reward都是0怎么办 ——&gt;  提供额外的reward去学习（Reward shaping）</p>
<p>就是通过人工加入规则外的reward（如射击游戏里浪费子弹就要扣分）——&gt; 需要设计者对问题本身有足够的理解</p>
<h4 id="Curiosity-based-reward-shaping"><a href="#Curiosity-based-reward-shaping" class="headerlink" title="Curiosity based reward shaping"></a>Curiosity based reward shaping</h4><p>要求agent看到新的东西，而且是有意义的新  就给他reward</p>
<hr>
<h2 id="No-Reward-Learning-from-Demonstration"><a href="#No-Reward-Learning-from-Demonstration" class="headerlink" title="No Reward: Learning from Demonstration"></a>No Reward: Learning from Demonstration</h2><p>定义Reward的方式非常困难，没定义好Reward会出现很坏的情况</p>
<h4 id="Imitation-Learning-模仿学习"><a href="#Imitation-Learning-模仿学习" class="headerlink" title="Imitation Learning 模仿学习"></a>Imitation Learning 模仿学习</h4><p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20251129132241028.png" alt="Imitation Learning示意图"></p>
<h4 id="Inversed-Reinforcement-Learning"><a href="#Inversed-Reinforcement-Learning" class="headerlink" title="Inversed Reinforcement Learning"></a>Inversed Reinforcement Learning</h4><p>本来不知道reward，通过expert示范去反推Reward Function</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20251129132828940.png" alt="Inversed Reinforcement Learning示意图"></p>
<p>简单的Reward Function可能能训出复杂的actor</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/image-20251129134748729.png" alt="Inversed Reinforcement Learning流程图"></p>
<p>先获得一些数据，然后找到一个reward function让expert的reward高于现有的actor，再让actor在这个reward function上学习</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7B6640D445-06F3-4C34-ADA9-5FABEF3B33DF%7D.png" alt="IRL框架"></p>
<h4 id="GAN-VS-IRL"><a href="#GAN-VS-IRL" class="headerlink" title="GAN VS IRL"></a>GAN VS IRL</h4><p>框架类似</p>
<p><img src="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%7B7ABC5C64-1072-4389-B207-44A434EB5533%7D.png" alt="GAN VS IRL示意图"></p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="https://avatars.githubusercontent.com/u/153807103" alt="Mrlaolu"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Mrlaolu</p><p class="is-size-6 is-block">Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Hangzhou</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives/"><p class="title">4</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories/"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags/"><p class="title">5</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Mrlaolu" target="_blank" rel="me noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/Mrlaolu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-12-15T11:04:36.558Z">2025-12-15</time></p><p class="title"><a href="/2025/12/15/Transformer%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">Transformer学习笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-12-12T02:20:57.813Z">2025-12-12</time></p><p class="title"><a href="/2025/12/12/ResNet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">ResNet学习笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-11-30T06:23:54.268Z">2025-11-30</time></p><p class="title"><a href="/2025/11/30/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9ACLIP/">论文笔记：CLIP</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-11-29T07:37:16.000Z">2025-11-29</time></p><p class="title"><a href="/2025/11/29/RL%E6%A6%82%E8%AE%BA%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">RL概论学习笔记</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/12/"><span class="level-start"><span class="level-item">十二月 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2025/11/"><span class="level-start"><span class="level-item">十一月 2025</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><span class="tag">学习笔记</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"><span class="tag">强化学习</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%BE%E5%86%85/"><span class="tag">课内</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="Mrlaolu&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2025 Mrlaolu</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script data-pjax src="/js/insight.js" defer></script><script data-pjax>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>